{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport numpy as np\nimport transformers\nfrom transformers import BertTokenizer\nfrom torch import nn\nfrom transformers import BertModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nimport re\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-12T15:04:52.199125Z","iopub.execute_input":"2023-03-12T15:04:52.200173Z","iopub.status.idle":"2023-03-12T15:04:52.210681Z","shell.execute_reply.started":"2023-03-12T15:04:52.200122Z","shell.execute_reply":"2023-03-12T15:04:52.209412Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-for-testing-yt-comms-bias/test_yt_comments.csv\n/kaggle/input/fine-tune-chat-gpt/learn_yt_comments fine-tune.csv\n/kaggle/input/yt-comments-bias-tips-normal/learn_yt_comments.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# THIS SCRIPT ANALYZES YOUTUBE COMMENTS\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False)\nlabels = {'extra': 0,\n          'normal': 1,\n          'question': 2,\n          'timestamps': 3,\n          'issue': 4\n          }\nlabels2 = ['extra', 'normal', 'question', 'timestamps', 'issue']\n\ndf = pd.read_csv(\"../input/fine-tune-chat-gpt/learn_yt_comments fine-tune.csv\")\ndf_test = pd.read_csv(\"../input/dataset-for-testing-yt-comms-bias/test_yt_comments.csv\")\n\ndf.head()\n\ni = 0\nfor text in df['text']:\n    text = re.sub(\"[.,!?-]\", '', text.lower())\n    text = re.sub(\"\\s{2,}\", ' ', text.lower())\n    df['text'][i] = text\n    i+=1\n\nnp.random.seed(75)\ndf_train, df_val = np.split(df.sample(frac=1, random_state=12), [int(.95*len(df))])\n\nprint(df_train.head())\nprint(df_train.at[2, 'category'])\n\nprint(len(df_train), len(df_val), len(df_test))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:11:45.267192Z","iopub.execute_input":"2023-03-12T15:11:45.267905Z","iopub.status.idle":"2023-03-12T15:11:46.897371Z","shell.execute_reply.started":"2023-03-12T15:11:45.267865Z","shell.execute_reply":"2023-03-12T15:11:46.896275Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"      category                                               text\n3494    normal  i thought the ai will cook the meal too and yo...\n2119    normal  excellent video i was lmao'ing all the way reg...\n9583  question  why are you constantly upselling academics emp...\n6535    normal  what an amazing discourse on the generalizatio...\n9293  question  the use of seeds is very interesting to me but...\nextra\n9301 490 1038\n","output_type":"stream"}]},{"cell_type":"code","source":"def text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    \n    text = text.lower()\n    text = re.sub(r\"what's\", \" what is \", text)\n    text = re.sub(r\"won't\", \" will not \", text)\n    text = re.sub(r\"([\\w\\d)])=\", r\"\\g<1> =\", text)  #  var=1 => var =1\n    text = re.sub(r\"=([\\w\\d)])\", r\"= \\g<1>\", text)  #  var=1 => var =1\n    text = re.sub(\"[\\(\\)]\", \" \", text) #  text) => text\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \" can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \" i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"-\", \" - \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    #text = ''.join(c for c in text if not c.isnumeric())\n    \n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:03:41.119857Z","iopub.execute_input":"2023-03-12T15:03:41.120441Z","iopub.status.idle":"2023-03-12T15:03:41.131029Z","shell.execute_reply.started":"2023-03-12T15:03:41.120390Z","shell.execute_reply":"2023-03-12T15:03:41.129833Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"for i in range(len(df_train)):\n    print(df_train.at[i, 'text'])\n    df_train.at[i, 'text'] = text_preprocessing(df_train.at[i, 'text'])\n    print(df_train.at[i, 'text'])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:03:44.931211Z","iopub.execute_input":"2023-03-12T15:03:44.931836Z","iopub.status.idle":"2023-03-12T15:03:44.938737Z","shell.execute_reply.started":"2023-03-12T15:03:44.931795Z","shell.execute_reply":"2023-03-12T15:03:44.937671Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":" if i remember correctly the net priority queue implementation is based on a heap structure (some can be based on tree structures instead) with a heap when you put a new item in you need to shift items down and for performance reasons you aren't moving items that have the same relative priority so when you insert a priority=1 after a priority=3 the 3 gets shifted down to make room for the 1 but only the first 3 will move so that means the first 3 you put in will end up after the second 3 you put in it's just the nature of the data structure used priority queues are generally used in places where you need things in priority order but not necessarily strict fifo order priority queues queues stacks and all sorts of other data structures have specific usecases and it's good to keep a bunch of structures in your toolbox so you can use the correct one at the right time\nif i remember correctly the net priority queue implementation is based on a heap structure some can be based on tree structures instead with a heap when you put a new item in you need to shift items down and for performance reasons you are not moving items that have the same relative priority so when you insert a priority = 1 after a priority = 3 the 3 gets shifted down to make room for the 1 but only the first 3 will move so that means the first 3 you put in will end up after the second 3 you put in it just the nature of the data structure used priority queues are generally used in places where you need things in priority order but not necessarily strict fifo order priority queues queues stacks and all sorts of other data structures have specific usecases and it good to keep a bunch of structures in your toolbox so you can use the correct one at the right time\n","output_type":"stream"}]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.labels = [labels[label] for label in df['category']]\n        self.texts = [tokenizer(text,\n                               padding='max_length', max_length = 128, truncation=True,\n                                return_tensors=\"pt\") for text in df['text']]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:03:53.723066Z","iopub.execute_input":"2023-03-12T15:03:53.724054Z","iopub.status.idle":"2023-03-12T15:03:53.732376Z","shell.execute_reply.started":"2023-03-12T15:03:53.724014Z","shell.execute_reply":"2023-03-12T15:03:53.730978Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.texts = [tokenizer(text,\n                               padding='max_length', max_length = 128, truncation=True,\n                                return_tensors=\"pt\") for text in df['text']]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n\n        return batch_texts","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:03:56.059846Z","iopub.execute_input":"2023-03-12T15:03:56.060206Z","iopub.status.idle":"2023-03-12T15:03:56.068207Z","shell.execute_reply.started":"2023-03-12T15:03:56.060174Z","shell.execute_reply":"2023-03-12T15:03:56.067108Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    \n    def __init__(self):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.linear = nn.Linear(768, 30)\n        self.linear2 = nn.Linear(30, 5)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask):\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask, return_dict=False)\n        linear_output = self.linear(pooled_output)\n        final_layer = self.relu(linear_output)\n        linear_output = self.linear2(final_layer)\n        return linear_output","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:58:12.590629Z","iopub.execute_input":"2023-03-12T15:58:12.591054Z","iopub.status.idle":"2023-03-12T15:58:12.614669Z","shell.execute_reply.started":"2023-03-12T15:58:12.591015Z","shell.execute_reply":"2023-03-12T15:58:12.611537Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"def train(model, train_data, val_data, learning_rate, epochs):\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    criterion = nn.CrossEntropyLoss()\n    \n    if use_cuda:\n        model = model.cuda()\n        \n    for epoch_num in range(epochs):\n\n        total_acc_train = 0\n        total_loss_train = 0\n        \n        optimizer = Adam(model.parameters(), lr=learning_rate)\n\n        for train_input, train_label in tqdm(train_dataloader):\n            train_label = train_label.to(device)\n            mask = train_input['attention_mask'].to(device)\n            input_id = train_input['input_ids'].squeeze(1).to(device)\n\n            output = model(input_id, mask)\n\n            batch_loss = criterion(output, train_label.long())\n            total_loss_train += batch_loss.item()\n\n            acc = (output.argmax(dim=1) == train_label).sum().item()\n            total_acc_train += acc\n\n            model.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n        total_acc_val = 0\n        total_loss_val = 0\n        \n\n        with torch.no_grad():\n            for val_input, val_label in val_dataloader:\n                val_label = val_label.to(device)\n                mask = val_input['attention_mask'].to(device)\n                input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)\n\n                batch_loss = criterion(output, val_label.long())\n                total_loss_val += batch_loss.item()\n\n                acc = (output.argmax(dim=1) == val_label).sum().item()\n                total_acc_val += acc\n\n        print(\n            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n        torch.save(model.state_dict(), \"../working/model_yt.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:58:44.648815Z","iopub.execute_input":"2023-03-12T15:58:44.649781Z","iopub.status.idle":"2023-03-12T15:58:44.662766Z","shell.execute_reply.started":"2023-03-12T15:58:44.649728Z","shell.execute_reply":"2023-03-12T15:58:44.661387Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 4\nLR = 3e-5\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = BertClassifier()\n#model.load_state_dict(torch.load(\"../working/model_yt.pt\"))\n\ntrain(model, df_train, df_val, LR, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:58:46.619561Z","iopub.execute_input":"2023-03-12T15:58:46.620105Z","iopub.status.idle":"2023-03-12T16:06:16.367527Z","shell.execute_reply.started":"2023-03-12T15:58:46.620062Z","shell.execute_reply":"2023-03-12T16:06:16.366486Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n100%|██████████| 291/291 [01:45<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Train Loss:  0.015                 | Train Accuracy:  0.887                 | Val Loss:  0.008                 | Val Accuracy:  0.927\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 291/291 [01:45<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Train Loss:  0.005                 | Train Accuracy:  0.961                 | Val Loss:  0.007                 | Val Accuracy:  0.935\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 291/291 [01:45<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Train Loss:  0.003                 | Train Accuracy:  0.979                 | Val Loss:  0.006                 | Val Accuracy:  0.943\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 291/291 [01:45<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Train Loss:  0.001                 | Train Accuracy:  0.990                 | Val Loss:  0.008                 | Val Accuracy:  0.943\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate(model, test_data):\n    test = Dataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    total_acc_test = 0\n    with torch.no_grad():\n        i = 0\n        for test_input, test_label in test_dataloader:\n            test_label = test_label.to(device)\n            mask = test_input['attention_mask'].to(device)\n            input_id = test_input['input_ids'].squeeze(1).to(device)\n\n            output = model(input_id, mask)\n\n            acc = (output.argmax(dim=1) == test_label).sum().item()\n            if labels2[output.argmax(dim=1).item()] != labels2[test_label.item()]:\n                print(labels2[output.argmax(dim=1).item()] + ',' + test_data.at[i, 'text'])\n            else:\n                print(test_data.at[i, 'category'] + ',' + test_data.at[i, 'text'])\n                #print(tokenizer.decode(test_dataloader.dataset.texts[i].input_ids[0])[:200], \"| IS | \", labels2[output.argmax(dim=1).item()], \" : \", labels2[test_label.item()])\n            total_acc_test += acc\n            i += 1\n\n    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:27:12.903324Z","iopub.execute_input":"2023-03-11T12:27:12.904526Z","iopub.status.idle":"2023-03-11T12:27:12.933065Z","shell.execute_reply.started":"2023-03-11T12:27:12.904481Z","shell.execute_reply":"2023-03-11T12:27:12.929992Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def evaluateAndPrintForModel(model, test_data):\n    test = Dataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    total_acc_test = 0\n    with torch.no_grad():\n        i = 0\n        for test_input, test_label in test_dataloader:\n            test_label = test_label.to(device)\n            mask = test_input['attention_mask'].to(device)\n            input_id = test_input['input_ids'].squeeze(1).to(device)\n\n            output = model(input_id, mask)\n\n            acc = (output.argmax(dim=1) == test_label).sum().item()\n            #print(output[0])\n            #print(output[0][0].item())\n            #print(output[0][1].item())\n            #print(output[0][2].item())\n            #print(output[0][3].item())\n            #print(output[0][4].item())\n            #return\n            if labels2[output.argmax(dim=1).item()] != labels2[test_label.item()]:\n                print(labels2[output.argmax(dim=1).item()] + ',' + test_data.at[i, 'text'])\n            else:\n                print(test_data.at[i, 'category'] + ',' + test_data.at[i, 'text'])\n                #print(tokenizer.decode(test_dataloader.dataset.texts[i].input_ids[0])[:200], \"| IS | \", labels2[output.argmax(dim=1).item()], \" : \", labels2[test_label.item()])\n            total_acc_test += acc\n            i += 1\n\n    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:27:16.395533Z","iopub.execute_input":"2023-03-11T12:27:16.396140Z","iopub.status.idle":"2023-03-11T12:27:16.407553Z","shell.execute_reply.started":"2023-03-11T12:27:16.396102Z","shell.execute_reply":"2023-03-11T12:27:16.406359Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def evaluateWithoutAccuracy(model, test_data):\n    test = TestDataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n    \n    resp = []\n    output = None\n    with torch.no_grad():\n        i = 0\n        for test_input in test_dataloader:\n            mask = test_input['attention_mask'].to(device)\n            input_id = test_input['input_ids'].squeeze(1).to(device)\n\n            output = model(input_id, mask)\n            print(output)\n            \n            #print(tokenizer.decode(test_dataloader.dataset.texts[i].input_ids[0])[:100], \" |||| \", labels2[output.argmax(dim=1).item()])\n            resp.append(tokenizer.decode(test_dataloader.dataset.texts[i].input_ids[0])[:200] + \" |||| \" + labels2[output.argmax(dim=1).item()])\n            i += 1\n    return [resp, output]","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:27:20.366829Z","iopub.execute_input":"2023-03-11T12:27:20.367559Z","iopub.status.idle":"2023-03-11T12:27:20.375915Z","shell.execute_reply.started":"2023-03-11T12:27:20.367521Z","shell.execute_reply":"2023-03-11T12:27:20.374709Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"evaluate(model,df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluateAndPrintForModel(model, df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_to_add = []","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:27:29.317900Z","iopub.execute_input":"2023-03-11T12:27:29.318279Z","iopub.status.idle":"2023-03-11T12:27:29.323205Z","shell.execute_reply.started":"2023-03-11T12:27:29.318246Z","shell.execute_reply":"2023-03-11T12:27:29.322072Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"while 1:\n    query = input(\"Text to check:\")\n    query = query.replace(',', '').replace('\\\"', '').replace('\\.', ' ').replace('\\n', '').replace('\\r', '').replace('\\t','').replace('  ', '').strip().lower()\n    if query == 'stop':\n        break\n    if len(query) < 10:\n        continue\n    df_test_input = pd.DataFrame({\"text\": [query]})\n    evaluated_resp, tensor = evaluateWithoutAccuracy(model, df_test_input)\n    evaluated_resp = evaluated_resp[0]\n    output = evaluated_resp.split(\"|||| \")[1]\n    print(output)\n    metric = torch.topk(tensor, 2)\n    firstOutput = metric[0][0][0].item()\n    secondOutput = metric[0][0][1].item()\n    if secondOutput == 0:\n        secondOutput = 1\n    howSure = firstOutput + secondOutput\n    howSure = firstOutput / howSure * 100\n    print(howSure)\n    if howSure < 70:\n        print(\"Sadly not sure...\")\n        continue\n    text = input(\"Correct? [y/n]\")\n    if text == 'y':\n        text_to_add.append(f'{output},{query}')\n        print(\"Added, curr len: \" + str(len(text_to_add)))\n    else:\n        new_output = input(\"New category:\")\n        text_to_add.append(f'{new_output},{query}')\n        print(\"Added, curr len: \" + str(len(text_to_add)))\n        \n        \n","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:27:30.737360Z","iopub.execute_input":"2023-03-11T12:27:30.737734Z","iopub.status.idle":"2023-03-11T14:27:48.814448Z","shell.execute_reply.started":"2023-03-11T12:27:30.737703Z","shell.execute_reply":"2023-03-11T14:27:48.813046Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdin","text":"Text to check: am i stealing art if i decide to copy a painting but paint it using a different technique\n"},{"name":"stdout","text":"tensor([[0.0000, 0.0000, 9.4130, 0.0000, 0.0000]], device='cuda:0')\nquestion\n90.39663993764908\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Correct? [y/n] \nNew category: \n"},{"name":"stdout","text":"Added, curr len: 1\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Text to check: not_sure,1:40 beta upscale it's the old engine we had it in v4 since the first day (and it's not the v4 engine not even nearly so perfect). we need a real v4 upscale!\n"},{"name":"stdout","text":"tensor([[0.0000, 9.5301, 0.0000, 0.0000, 0.0000]], device='cuda:0')\nnormal\n90.5034130442402\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Correct? [y/n] \nNew category: \n"},{"name":"stdout","text":"Added, curr len: 2\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Text to check: stop\n"}]},{"cell_type":"code","source":"for text in text_to_add:\n    query = text.replace('\\\"', '').replace('\\.', ' ').replace('\\n', '').replace('\\r', '').replace('\\t','').replace('  ', '').strip().lower()\n    print(query)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T17:34:53.819720Z","iopub.execute_input":"2023-03-06T17:34:53.820388Z","iopub.status.idle":"2023-03-06T17:34:53.827816Z","shell.execute_reply.started":"2023-03-06T17:34:53.820351Z","shell.execute_reply":"2023-03-06T17:34:53.826731Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"normal,i want to try runway. i think it is an awesome software!\nquestion,does gpt 3 require a paid account? i tried to access it to no avail.\nquestion,very nice content. thanks for the research. i would like to know if there is an ai application for predicting the next set of a group of numbers?or one that helps to quickly develop mobile apps?\nquestion,this is so interesting!! it’s insane to see how technology develops. but would these ais like bhuman be able to read non-english names correctly or is that something to wait for?\nquestion,quick question if anyone knows: how is the pricing determined on these illustrations? do you set your own or does adobe and the other ones which accept ai generated images decide?\nextra,you also need a midjourney commercial license to sell the images generated there. otherwise they fall under the creative commons noncommercial 4.0 attribution international license (the “asset license”).\nquestion,this is awesome thank you matt. question - do you know how i could change my png file to jpg with a maximum size of 45mb and minimum resolution of 4mp. i appears when i try to load my art to wirestock it tells me that they do not meet the requirements. however i am using the same files that i loaded into adobe stock successfully.\nextra,i tried a ton of the image upscalers and settled on the exact same ones as you upscale media for free and gigapixel for paid. for the free one i got best results by doing 2x with enhance and then 2x without.great tutorial ty\nquestion,this is so cool. thank you matt! hey two questions. one if i upload images to one or more of these sites can i still use them too? two you said adobe stock was the only site to let you use ai generative art but then you used that wire-thing site to upload to shutterstock etc. do all those sites know it's ai art and are okay with it? (bonus question: did you have to create an account on all those other sites first?) thanks again!\nnormal,thanks for the detailed walkthrough mate! i love creating ai art so may as well try make some dollars with it. i'm sure overtime it will add up\nquestion,as the legal things are so confusing to a layman like me i have understood that you can only sell images created by the lisenced/paid version of midjourney. how about bluewillow which creates quite different style pictures? any knowledge of that?\nquestion,love your channel btw!can you use a prompt that would reference the style of a deceased artist.such as create a brain in the style of picasso with books and . . . would that be a usage infringement?\nquestion,how much does adobe charge for stock photos and what percent do you get from the sell? do you have to remove any watermarks?\nquestion,thanks for all your brilliant content and future tools! do you use a free photo editor (shown in this video)? if so which one?\nquestion,do i have to pay a monthly fee for adobe stock if i only want to upload and sell my own images?\nquestion,this is pretty cool!!does anyone know if there is a way to add your signature to the lower left/right side of these images.not large of course but just so it's there.thanks in advance either way if anyone knows!\nquestion,hey matt!you're videos are great and i really appreciate them all.can you tell me out of all these ways of making money with ai would be the most profitable to start with?thank you!\nnormal,dropped and broke display screen on the cement. spent 2 days looking for solutions to transfer the data as i did not have some photos and videos get backed up. tried many alternatives with fail until i found your video. this happened last thursday and just ordered the usb c hub adapter today saturday. should be here tuesday so feeling hopeful this will be the key to the solution i have been looking for. many many thanks for creating this video and i am sure it is indeed loads of help to folks like me. again thank you.\nquestion,wow! i'm really impressed that you were able to generate a summary from a 117-page pdf. i've been hoping to use chatgpt to read long reports like this. can you tell me how you did it? i don't know anything about coding but i was wondering if you could develop this summary function into a standalone software?\nquestion,on different topic do you know of good resources and examples to apply sequence prediction maybe using directed graphs like product recommendation / next event prediction?\nquestion,did you find the answer to why the train dataset is smaller than the test? i've encountered the same problem with every graph dataset i've used. although one thing i did notice is that the validation accuracy did not increase when i used the larger one as train in any of the graphs and in a couple of cases it even decreased.\n","output_type":"stream"}]}]}